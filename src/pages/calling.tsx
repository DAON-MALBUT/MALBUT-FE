import { useState, useEffect, useRef } from 'react';
import { useNavigate, useLocation } from 'react-router-dom';
import MobileLayout from '@/layouts/mobile';
import { callApi, characterApi, apiClient, elevenLabsApi } from '@/api/client';
import { CallWebSocket, base64ToBlob } from '@/utils/websocket';
import type { 
  TranscriptionMessage, 
  AIResponseTextMessage, 
  AIResponseAudioMessage 
} from '@/utils/websocket';

interface CallingState {
  characterId?: string;
  characterName?: string;
  characterImage?: string;
  phoneNumber?: string;
  voiceId?: string;
}

export default function Calling() {
  const navigate = useNavigate();
  const location = useLocation();
  const state = location.state as CallingState;
  
  const [callDuration, setCallDuration] = useState(0);
  const [isMuted, setIsMuted] = useState(false);
  const [isSpeakerOn, setIsSpeakerOn] = useState(false);
  const [isConnected, setIsConnected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isAiSpeaking, setIsAiSpeaking] = useState(false);
  const [transcription, setTranscription] = useState('');
  const [aiResponse, setAiResponse] = useState('');
  
  const audioRef = useRef<HTMLAudioElement>(null);
  const wsRef = useRef<CallWebSocket | null>(null);
  const sessionIdRef = useRef<string | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioStreamRef = useRef<MediaStream | null>(null);
  const callStartTimeRef = useRef<Date | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const silenceTimeoutRef = useRef<number | null>(null);
  const canListenRef = useRef<boolean>(true);

  const characterId = state?.characterId || '';
  const characterName = state?.characterName || 'ì•Œ ìˆ˜ ì—†ìŒ';
  const characterImage = state?.characterImage || '';
  const phoneNumber = state?.phoneNumber || '';

  // í†µí™” ì„¸ì…˜ ì‹œì‘ ë° WebSocket ì—°ê²°
  useEffect(() => {
    const initCall = async () => {
      try {
        console.log('ğŸ“ Starting call session...');
        const response = await callApi.startSession(characterId || undefined);
        console.log('âœ… Session created:', response);
        
        sessionIdRef.current = response.session_id;
        callStartTimeRef.current = new Date();

        const baseUrl = apiClient.defaults.baseURL || '';
        console.log('ğŸŒ Base URL:', baseUrl);
        
        const ws = new CallWebSocket(baseUrl);
        wsRef.current = ws;

        await ws.connect(response.session_id);
        console.log('âœ… WebSocket connected');
        setIsConnected(true);

        ws.on('transcription', (msg) => {
          const transcriptMsg = msg as TranscriptionMessage;
          console.log('ğŸ“ Transcription received:', transcriptMsg);
          if (transcriptMsg.is_final) {
            setTranscription(transcriptMsg.text);
          }
        });

        ws.on('ai_response_text', (msg) => {
          const textMsg = msg as AIResponseTextMessage;
          console.log('ğŸ’¬ AI Response Text:', textMsg.text);
          setAiResponse(textMsg.text);
        });

        ws.on('ai_response_audio', async (msg) => {
          const audioMsg = msg as AIResponseAudioMessage;
          console.log('ğŸ”Š AI Response Audio received:', audioMsg.audio_url);
          
          // AI ì‘ë‹µ ì‹œì‘ - ë§ˆì´í¬ ë¹„í™œì„±í™”
          canListenRef.current = false;
          setIsAiSpeaking(true);
          
          // ì§„í–‰ ì¤‘ì¸ ë…¹ìŒ ì¤‘ì§€
          if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
            mediaRecorderRef.current.stop();
            audioChunksRef.current = []; // ë…¹ìŒ ë°ì´í„° ë²„ë¦¬ê¸°
          }
          
          if (audioRef.current && audioMsg.audio_data) {
            const blob = base64ToBlob(audioMsg.audio_data);
            const url = URL.createObjectURL(blob);
            audioRef.current.src = url;
            
            // ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ í›„ ë§ˆì´í¬ ë‹¤ì‹œ í™œì„±í™”
            audioRef.current.onended = () => {
              console.log('âœ… AI finished speaking');
              setIsAiSpeaking(false);
              canListenRef.current = true;
            };
            
            await audioRef.current.play().catch(err => {
              console.error('Audio play failed:', err);
              setIsAiSpeaking(false);
              canListenRef.current = true;
            });
          }
        });

        ws.on('error', (msg) => {
          console.error('âŒ WebSocket error:', msg);
        });

        ws.on('*', (msg) => {
          console.log('ğŸ“¨ WebSocket message:', msg);
        });

        await startRecording(ws);
      } catch (error) {
        console.error('âŒ Failed to start call:', error);
        
        let errorMessage = 'í†µí™” ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.';
        if (error instanceof Error) {
          if (error.message.includes('timeout')) {
            errorMessage = 'ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼. ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.';
          } else if (error.message.includes('closed')) {
            errorMessage = 'WebSocket ì—°ê²°ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.';
          } else {
            errorMessage = `ì—°ê²° ì‹¤íŒ¨: ${error.message}`;
          }
        }
        
        alert(errorMessage);
        navigate('/character');
      }
    };

    initCall();

    return () => {
      if (wsRef.current) {
        wsRef.current.stopListening();
        wsRef.current.close();
      }
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop();
      }
      if (audioStreamRef.current) {
        audioStreamRef.current.getTracks().forEach(track => track.stop());
      }
    };
  }, [characterId, navigate]);

  // í†µí™” ì‹œê°„ íƒ€ì´ë¨¸
  useEffect(() => {
    if (!isConnected) return;
    
    const timer = setInterval(() => {
      setCallDuration((prev) => prev + 1);
    }, 1000);

    return () => clearInterval(timer);
  }, [isConnected]);

  // í†µí™” ì‹œê°„ í¬ë§·íŒ… (00:00)
  const formatDuration = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  // ë§ˆì´í¬ ì„¤ì • ë° ìŒì„± ê°ì§€ ì‹œì‘
  const startRecording = async (ws: CallWebSocket) => {
    try {
      console.log('ğŸ¤ Requesting microphone access...');
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          channelCount: 1,
          sampleRate: 48000,
          echoCancellation: true,
          noiseSuppression: true,
        } 
      });
      audioStreamRef.current = stream;
      console.log('âœ… Microphone access granted');

      // Web Audio APIë¡œ ìŒì„± ê°ì§€ ì„¤ì •
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 2048;
      source.connect(analyser);

      const dataArray = new Uint8Array(analyser.frequencyBinCount);

      // ìŒì„± ë ˆë²¨ ê°ì§€
      const checkAudioLevel = () => {
        analyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;

        // ì£¼ê¸°ì ìœ¼ë¡œ ë ˆë²¨ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
        if (Math.random() < 0.01) { // 1% í™•ë¥ ë¡œ ë¡œê·¸
          console.log(`ğŸ”Š Audio level: ${average.toFixed(2)}, canListen: ${canListenRef.current}, recording: ${mediaRecorderRef.current?.state}`);
        }

        // AIê°€ ë§í•˜ëŠ” ì¤‘ì´ë©´ ë…¹ìŒ ì‹œì‘ ì•ˆ í•¨
        if (!canListenRef.current) {
          requestAnimationFrame(checkAudioLevel);
          return;
        }

        // ìŒì„±ì´ ê°ì§€ë˜ë©´ ë…¹ìŒ ì‹œì‘
        if (average > 10 && !mediaRecorderRef.current) {
          console.log(`ğŸ¤ Voice detected! Level: ${average.toFixed(2)}`);
          startRecordingChunk(stream, ws);
        }

        if (mediaRecorderRef.current?.state === 'recording') {
          // ì¹¨ë¬µ ê°ì§€ (2ì´ˆ)
          if (average < 10) {
            if (!silenceTimeoutRef.current) {
              console.log('ğŸ¤« Silence detected, will stop in 2 seconds...');
              silenceTimeoutRef.current = window.setTimeout(() => {
                stopRecordingAndSend(ws);
              }, 2000);
            }
          } else {
            // ìŒì„± ì¬ê°ì§€ ì‹œ íƒ€ì´ë¨¸ ë¦¬ì…‹
            if (silenceTimeoutRef.current) {
              console.log('ğŸ—£ï¸ Voice resumed, canceling silence timer');
              clearTimeout(silenceTimeoutRef.current);
              silenceTimeoutRef.current = null;
            }
          }
        }

        requestAnimationFrame(checkAudioLevel);
      };

      checkAudioLevel();
      ws.startListening('ko-KR', 48000);
      console.log('ğŸš€ Voice detection started');
    } catch (error) {
      console.error('âŒ Failed to start recording:', error);
      alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
    }
  };

  // ë…¹ìŒ ì²­í¬ ì‹œì‘
  const startRecordingChunk = (stream: MediaStream, ws: CallWebSocket) => {
    if (mediaRecorderRef.current) return;

    console.log('ğŸ™ï¸ Recording started');
    setIsRecording(true);
    audioChunksRef.current = [];

    // ë¸Œë¼ìš°ì €ê°€ ì§€ì›í•˜ëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ ì°¾ê¸°
    const mimeTypes = [
      'audio/webm;codecs=opus',
      'audio/webm',
      'audio/ogg;codecs=opus',
      'audio/mp4',
    ];
    
    const supportedMimeType = mimeTypes.find(type => MediaRecorder.isTypeSupported(type));
    console.log('ğŸµ Using MIME type:', supportedMimeType);

    const mediaRecorder = new MediaRecorder(stream, {
      mimeType: supportedMimeType,
      audioBitsPerSecond: 128000,
    });
    mediaRecorderRef.current = mediaRecorder;

    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunksRef.current.push(event.data);
      }
    };

    mediaRecorder.onstop = async () => {
      console.log('â¹ï¸ Recording stopped');
      setIsRecording(false);
      
      if (audioChunksRef.current.length > 0) {
        const mimeType = mediaRecorder.mimeType;
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType });
        console.log(`ğŸ“¤ Audio recorded: ${audioBlob.size} bytes, type: ${mimeType}`);
        
        if (!ws.isConnected()) {
          console.error('âŒ WebSocket not connected');
        } else {
          try {
            // ElevenLabs STTë¡œ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            console.log('ğŸ¯ Transcribing with ElevenLabs STT...');
            const transcribedText = await elevenLabsApi.speechToText(audioBlob);
            console.log('âœ… Transcription result:', transcribedText);
            
            if (transcribedText.trim()) {
              // í…ìŠ¤íŠ¸ë¥¼ WebSocketìœ¼ë¡œ ì „ì†¡
              ws.sendTextInput(transcribedText);
              setTranscription(transcribedText);
            } else {
              console.warn('âš ï¸ Empty transcription result');
            }
          } catch (error) {
            console.error('âŒ STT failed:', error);
          }
        }
        
        audioChunksRef.current = [];
      } else {
        console.warn('âš ï¸ No audio chunks recorded');
      }
      
      mediaRecorderRef.current = null;
    };

    mediaRecorder.start();
  };

  // ë…¹ìŒ ì¤‘ì§€ ë° ì „ì†¡
  const stopRecordingAndSend = (_ws: CallWebSocket) => {
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current);
      silenceTimeoutRef.current = null;
    }

    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
  };

  const handleEndCall = async () => {
    if (sessionIdRef.current) {
      const duration = callStartTimeRef.current 
        ? Math.floor((Date.now() - callStartTimeRef.current.getTime()) / 1000)
        : callDuration;

      // ì„¸ì…˜ ì¢…ë£Œ
      try {
        await callApi.endSession(sessionIdRef.current);
        console.log('âœ… Call session ended');
      } catch (error) {
        console.error('âŒ Failed to end session:', error);
      }
      
      // ì‚¬ìš© ë¡œê¹… (ì„ íƒì , ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ)
      if (characterId) {
        try {
          await characterApi.logUsage(characterId, duration);
          console.log('âœ… Usage logged');
        } catch (error) {
          console.warn('âš ï¸ Failed to log usage (optional):', error);
        }
      }
    }
    
    navigate('/home');
  };

  const handleToggleMute = () => {
    setIsMuted(!isMuted);
    
    if (audioStreamRef.current) {
      audioStreamRef.current.getAudioTracks().forEach(track => {
        track.enabled = isMuted;
      });
    }
  };

  const handleToggleSpeaker = () => {
    setIsSpeakerOn(!isSpeakerOn);
  };

  const handleStopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      console.log('â¹ï¸ User stopped recording');
      mediaRecorderRef.current.stop();
      audioChunksRef.current = []; // ë…¹ìŒ ë°ì´í„° ë²„ë¦¬ê¸°
      
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
        silenceTimeoutRef.current = null;
      }
    }
  };

  return (
    <MobileLayout showNavBar={false}>
      <div 
        className="absolute inset-0 -mx-6 -mt-11 flex flex-col items-center justify-between"
        style={{
          background: 'linear-gradient(180deg, rgba(30,30,30,0.5) 0%, rgba(30,30,30,0.5) 100%)',
        }}
      >
        {/* ë°°ê²½ ì´ë¯¸ì§€ (ë¸”ëŸ¬ ì²˜ë¦¬) */}
        <div 
          className="absolute inset-0 -z-10"
          style={{
            backgroundImage: characterImage ? `url(${characterImage})` : 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)',
            backgroundSize: 'cover',
            backgroundPosition: 'center',
            filter: 'blur(40px)',
            transform: 'scale(1.2)',
          }}
        />

        {/* ìƒë‹¨ ì˜ì—­ - ë°œì‹ ì ì •ë³´ */}
        <div className="flex flex-col items-center pt-32 z-10 flex-1">
          {/* í”„ë¡œí•„ ì´ë¯¸ì§€ */}
          <div className="w-[120px] h-[120px] rounded-full bg-white flex items-center justify-center overflow-hidden shadow-2xl mb-6">
            {characterImage ? (
              <img
                src={characterImage}
                alt={characterName}
                className="w-full h-full object-cover"
              />
            ) : (
              <span className="text-[48px] text-[#FF7038] font-bold">
                {characterName[0]}
              </span>
            )}
          </div>

          {/* ë°œì‹ ì ì´ë¦„ */}
          <h1 className="text-[32px] font-bold text-white mb-2 drop-shadow-lg">
            {characterName}
          </h1>

          {/* ì „í™”ë²ˆí˜¸ */}
          {phoneNumber && (
            <p className="text-[18px] text-white opacity-70 mb-4 drop-shadow-md">
              {phoneNumber}
            </p>
          )}

          {/* í†µí™” ì‹œê°„ */}
          <p className="text-[24px] font-semibold text-white drop-shadow-lg">
            {formatDuration(callDuration)}
          </p>

          {/* ì‹¤ì‹œê°„ ì „ì‚¬ ë° AI ì‘ë‹µ í‘œì‹œ */}
          {(transcription || aiResponse || isRecording || isAiSpeaking) && (
            <div className="mt-8 px-8 max-w-md">
              {isRecording && (
                <div className="bg-red-500 bg-opacity-30 backdrop-blur-sm rounded-2xl p-4 mb-3 animate-pulse">
                  <p className="text-[14px] text-white opacity-90">
                    <span className="font-semibold">ğŸ™ï¸ ë…¹ìŒ ì¤‘...</span>
                  </p>
                </div>
              )}
              {transcription && (
                <div className="bg-white bg-opacity-20 backdrop-blur-sm rounded-2xl p-4 mb-3">
                  <p className="text-[14px] text-white opacity-90">
                    <span className="font-semibold">ì‚¬ìš©ì:</span> {transcription}
                  </p>
                </div>
              )}
              {isAiSpeaking && (
                <div className="bg-blue-500 bg-opacity-30 backdrop-blur-sm rounded-2xl p-4 mb-3 animate-pulse">
                  <p className="text-[14px] text-white opacity-90">
                    <span className="font-semibold">ğŸ”Š {characterName} ë§í•˜ëŠ” ì¤‘...</span>
                  </p>
                </div>
              )}
              {aiResponse && !isAiSpeaking && (
                <div className="bg-white bg-opacity-20 backdrop-blur-sm rounded-2xl p-4">
                  <p className="text-[14px] text-white opacity-90">
                    <span className="font-semibold">{characterName}:</span> {aiResponse}
                  </p>
                </div>
              )}
            </div>
          )}
        </div>

        {/* ìˆ¨ê²¨ì§„ ì˜¤ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ */}
        <audio ref={audioRef} hidden />

        {/* í•˜ë‹¨ ì˜ì—­ - ì»¨íŠ¸ë¡¤ ë²„íŠ¼ */}
        <div className="w-full pb-20 z-10">
          {/* ì¸ë””ì¼€ì´í„° */}
          <div className="flex justify-center mb-6">
            <div className="w-9 h-1 bg-white rounded-full opacity-60" />
          </div>

          {/* ë²„íŠ¼ ê·¸ë£¹ */}
          <div className="flex items-center justify-center gap-6 px-8 mb-8">
            {/* ìŒì†Œê±° ë²„íŠ¼ */}
            <div className="flex flex-col items-center gap-2">
              <button
                onClick={handleToggleMute}
                className={`w-14 h-14 rounded-full flex items-center justify-center transition-all shadow-lg ${
                  isMuted ? 'bg-[#838080] bg-opacity-100' : 'bg-white bg-opacity-16'
                }`}
                aria-label="ìŒì†Œê±°"
              >
                <img
                  src={isMuted ? "/icon/no_mike.svg" : "/icon/mike.svg"}
                  alt="ìŒì†Œê±°"
                  className="w-7 h-7 transition-all duration-300"
                  style={{
                    filter: isMuted ? 'brightness(0) invert(1)' : 'brightness(0) saturate(100%) invert(60%) sepia(0%) saturate(0%)'
                  }}
                />
              </button>
              <span className="text-[12px] text-white font-normal">
                ìŒì†Œê±°
              </span>
            </div>

            {/* ë…¹ìŒ ì¤‘ë‹¨ ë²„íŠ¼ (ë…¹ìŒ ì¤‘ì¼ ë•Œë§Œ í‘œì‹œ) */}
            {isRecording && (
              <div className="flex flex-col items-center gap-2">
                <button
                  onClick={handleStopRecording}
                  className="w-14 h-14 bg-red-500 rounded-full flex items-center justify-center transition-all shadow-lg hover:bg-red-600 active:scale-95 animate-pulse"
                  aria-label="ë…¹ìŒ ì¤‘ë‹¨"
                >
                  <div className="w-4 h-4 bg-white rounded-sm" />
                </button>
                <span className="text-[12px] text-white font-normal">
                  ì¤‘ë‹¨
                </span>
              </div>
            )}

            {/* ìŠ¤í”¼ì»¤ ë²„íŠ¼ */}
            <div className="flex flex-col items-center gap-2">
              <button
                onClick={handleToggleSpeaker}
                className={`w-14 h-14 rounded-full flex items-center justify-center transition-all duration-300 shadow-lg ${
                  isSpeakerOn
                    ? 'bg-[#838080] bg-opacity-100'
                    : 'bg-white bg-opacity-16'
                }`}
                aria-label="ìŠ¤í”¼ì»¤"
              >
                <img
                  src="/icon/speacker.svg"
                  alt="ìŠ¤í”¼ì»¤"
                  className="w-7 h-7 transition-all duration-300"
                  style={{
                    filter: isSpeakerOn ? 'brightness(0) invert(1)' : 'brightness(0) saturate(100%) invert(60%) sepia(0%) saturate(0%)'
                  }}
                />
              </button>
              <span className="text-[12px] text-white font-normal">
                ìŠ¤í”¼ì»¤
              </span>
            </div>

            {/* ì¢…ë£Œ ë²„íŠ¼ */}
            <div className="flex flex-col items-center gap-2">
              <button
                onClick={handleEndCall}
                className="w-14 h-14 bg-[#EB5545] rounded-full flex items-center justify-center transition-all shadow-lg hover:bg-[#D94A3C] active:scale-95"
                aria-label="í†µí™” ì¢…ë£Œ"
              >
                <img
                  src="/icon/end.svg"
                  alt="ì¢…ë£Œ"
                  className="w-8 h-8"
                  style={{ filter: 'brightness(0) invert(1)' }}
                />
              </button>
              <span className="text-[12px] text-white font-normal">
                end
              </span>
            </div>
          </div>
        </div>
      </div>
    </MobileLayout>
  );
}
