import { useState, useEffect, useRef } from 'react';
import { useNavigate, useLocation } from 'react-router-dom';
import SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition';
import MobileLayout from '@/layouts/mobile';
import { callApi, characterApi, apiClient } from '@/api/client';
import { CallWebSocket, base64ToBlob } from '@/utils/websocket';
import type { 
  TranscriptionMessage, 
  AIResponseTextMessage, 
  AIResponseAudioMessage 
} from '@/utils/websocket';

interface CallingState {
  characterId?: string;
  characterName?: string;
  characterImage?: string;
  phoneNumber?: string;
  voiceId?: string;
}

export default function Calling() {
  const navigate = useNavigate();
  const location = useLocation();
  const state = location.state as CallingState;
  
  const {
    transcript,
    finalTranscript,
    interimTranscript,
    listening,
    resetTranscript,
    browserSupportsSpeechRecognition
  } = useSpeechRecognition();
  
  const [callDuration, setCallDuration] = useState(0);
  const [isMuted, setIsMuted] = useState(false);
  const [isSpeakerOn, setIsSpeakerOn] = useState(false);
  const [isConnected, setIsConnected] = useState(false);
  const [isAiSpeaking, setIsAiSpeaking] = useState(false);
  const [transcription, setTranscription] = useState('');
  const [aiResponse, setAiResponse] = useState('');
  
  const audioRef = useRef<HTMLAudioElement>(null);
  const wsRef = useRef<CallWebSocket | null>(null);
  const sessionIdRef = useRef<string | null>(null);
  const audioStreamRef = useRef<MediaStream | null>(null);
  const callStartTimeRef = useRef<Date | null>(null);
  const canListenRef = useRef<boolean>(true);
  const lastSentTranscriptRef = useRef<string>('');
  const silenceTimerRef = useRef<number | null>(null);

  const characterId = state?.characterId || '';
  const characterName = state?.characterName || 'ì•Œ ìˆ˜ ì—†ìŒ';
  const characterImage = state?.characterImage || '';
  const phoneNumber = state?.phoneNumber || '';

  // ë¸Œë¼ìš°ì €ê°€ ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°
  useEffect(() => {
    if (!browserSupportsSpeechRecognition) {
      alert('ë¸Œë¼ìš°ì €ê°€ ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
      navigate('/home');
    }
  }, [browserSupportsSpeechRecognition, navigate]);

  // transcript ë³€ê²½ ê°ì§€ ë° ì „ì†¡
  useEffect(() => {
    if (!wsRef.current || !canListenRef.current || isAiSpeaking) return;

    // interim transcriptê°€ ìˆìœ¼ë©´ í™”ë©´ì— í‘œì‹œ (ì•„ì§ ì „ì†¡í•˜ì§€ ì•ŠìŒ)
    if (interimTranscript) {
      console.log('ğŸ¤ Interim:', interimTranscript);
      setTranscription(transcript); // ì „ì²´ transcript í‘œì‹œ (final + interim)
      
      // ì¹¨ë¬µ íƒ€ì´ë¨¸ ë¦¬ì…‹
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
      
      // 1.5ì´ˆ ì¹¨ë¬µ í›„ ì „ì†¡
      silenceTimerRef.current = window.setTimeout(() => {
        if (transcript && transcript !== lastSentTranscriptRef.current) {
          console.log('ğŸ“¤ Sending after silence:', transcript);
          wsRef.current?.sendTextInput(transcript);
          lastSentTranscriptRef.current = transcript;
          setTranscription(transcript);
          resetTranscript();
        }
      }, 1800);
    }
    
    // final transcriptê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì „ì†¡
    if (finalTranscript && finalTranscript !== lastSentTranscriptRef.current) {
      console.log('âœ… Final transcript:', finalTranscript);
      
      // ì¹¨ë¬µ íƒ€ì´ë¨¸ ì·¨ì†Œ
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
      
      wsRef.current.sendTextInput(finalTranscript);
      lastSentTranscriptRef.current = finalTranscript;
      setTranscription(finalTranscript);
      
      // ì•½ê°„ì˜ ì§€ì—° í›„ ë¦¬ì…‹ (ë‹¤ìŒ ì…ë ¥ ì¤€ë¹„)
      setTimeout(() => {
        resetTranscript();
        lastSentTranscriptRef.current = '';
      }, 500);
    }
  }, [transcript, finalTranscript, interimTranscript, isAiSpeaking, resetTranscript]);

  // í†µí™” ì„¸ì…˜ ì‹œì‘ ë° WebSocket ì—°ê²°
  useEffect(() => {
    const initCall = async () => {
      try {
        console.log('ğŸ“ Starting call session...');
        const response = await callApi.startSession(characterId || undefined);
        console.log('âœ… Session created:', response);
        
        sessionIdRef.current = response.session_id;
        callStartTimeRef.current = new Date();

        const baseUrl = apiClient.defaults.baseURL || '';
        console.log('ğŸŒ Base URL:', baseUrl);
        
        const ws = new CallWebSocket(baseUrl);
        wsRef.current = ws;

        await ws.connect(response.session_id);
        console.log('âœ… WebSocket connected');
        setIsConnected(true);

        ws.on('transcription', (msg) => {
          const transcriptMsg = msg as TranscriptionMessage;
          console.log('ğŸ“ Transcription received:', transcriptMsg);
          if (transcriptMsg.is_final) {
            setTranscription(transcriptMsg.text);
          }
        });

        ws.on('ai_response_text', (msg) => {
          const textMsg = msg as AIResponseTextMessage;
          console.log('ğŸ’¬ AI Response Text:', textMsg.text);
          setAiResponse(textMsg.text);
        });

        ws.on('ai_response_audio', async (msg) => {
          const audioMsg = msg as AIResponseAudioMessage;
          console.log('ğŸ”Š AI Response Audio received:', audioMsg.audio_url);
          
          // AI ì‘ë‹µ ì‹œì‘ - ìŒì„± ì¸ì‹ ì¤‘ì§€
          canListenRef.current = false;
          setIsAiSpeaking(true);
          SpeechRecognition.stopListening();
          resetTranscript();
          
          if (audioRef.current && audioMsg.audio_data) {
            const blob = base64ToBlob(audioMsg.audio_data);
            const url = URL.createObjectURL(blob);
            audioRef.current.src = url;
            
            // ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ í›„ ìŒì„± ì¸ì‹ ë‹¤ì‹œ ì‹œì‘
            audioRef.current.onended = () => {
              console.log('âœ… AI finished speaking');
              setIsAiSpeaking(false);
              canListenRef.current = true;
              if (!isMuted) {
                SpeechRecognition.startListening({ 
                  language: 'ko-KR', 
                  continuous: true 
                });
              }
            };
            
            await audioRef.current.play().catch(err => {
              console.error('Audio play failed:', err);
              setIsAiSpeaking(false);
              canListenRef.current = true;
              if (!isMuted) {
                SpeechRecognition.startListening({ 
                  language: 'ko-KR', 
                  continuous: true 
                });
              }
            });
          }
        });

        ws.on('error', (msg) => {
          console.error('âŒ WebSocket error:', msg);
        });

        ws.on('*', (msg) => {
          console.log('ğŸ“¨ WebSocket message:', msg);
        });

        await startMicrophone();
      } catch (error) {
        console.error('âŒ Failed to start call:', error);
        
        let errorMessage = 'í†µí™” ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.';
        if (error instanceof Error) {
          if (error.message.includes('timeout')) {
            errorMessage = 'ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼. ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.';
          } else if (error.message.includes('closed')) {
            errorMessage = 'WebSocket ì—°ê²°ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.';
          } else {
            errorMessage = `ì—°ê²° ì‹¤íŒ¨: ${error.message}`;
          }
        }
        
        alert(errorMessage);
        navigate('/character');
      }
    };

    initCall();

    return () => {
      if (wsRef.current) {
        wsRef.current.stopListening();
        wsRef.current.close();
      }
      SpeechRecognition.stopListening();
      if (audioStreamRef.current) {
        audioStreamRef.current.getTracks().forEach(track => track.stop());
      }
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
      }
    };
  }, [characterId, navigate]);

  // í†µí™” ì‹œê°„ íƒ€ì´ë¨¸
  useEffect(() => {
    if (!isConnected) return;
    
    const timer = setInterval(() => {
      setCallDuration((prev) => prev + 1);
    }, 1000);

    return () => clearInterval(timer);
  }, [isConnected]);

  // í†µí™” ì‹œê°„ í¬ë§·íŒ… (00:00)
  const formatDuration = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ë° ìŒì„± ì¸ì‹ ì‹œì‘
  const startMicrophone = async () => {
    try {
      console.log('ğŸ¤ Requesting microphone access...');
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          channelCount: 1,
          sampleRate: 48000,
          echoCancellation: true,
          noiseSuppression: true,
        } 
      });
      audioStreamRef.current = stream;
      console.log('âœ… Microphone access granted');

      // ìŒì„± ì¸ì‹ ì‹œì‘
      if (wsRef.current) {
        wsRef.current.startListening('ko-KR', 48000);
      }
      
      SpeechRecognition.startListening({ 
        language: 'ko-KR', 
        continuous: true 
      });
      console.log('ğŸš€ Speech recognition started');
    } catch (error) {
      console.error('âŒ Failed to start microphone:', error);
      alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
    }
  };

  const handleEndCall = async () => {
    if (sessionIdRef.current) {
      const duration = callStartTimeRef.current 
        ? Math.floor((Date.now() - callStartTimeRef.current.getTime()) / 1000)
        : callDuration;

      // ì„¸ì…˜ ì¢…ë£Œ
      try {
        await callApi.endSession(sessionIdRef.current);
        console.log('âœ… Call session ended');
      } catch (error) {
        console.error('âŒ Failed to end session:', error);
      }
      
      // ì‚¬ìš© ë¡œê¹… (ì„ íƒì , ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ)
      if (characterId) {
        try {
          await characterApi.logUsage(characterId, duration);
          console.log('âœ… Usage logged');
        } catch (error) {
          console.warn('âš ï¸ Failed to log usage (optional):', error);
        }
      }
    }
    
    navigate('/home');
  };

  const handleToggleMute = () => {
    setIsMuted(!isMuted);
    
    if (audioStreamRef.current) {
      audioStreamRef.current.getAudioTracks().forEach(track => {
        track.enabled = isMuted;
      });
    }
    
    // ìŒì†Œê±° í† ê¸€
    if (isMuted) {
      // ìŒì†Œê±° í•´ì œ - ìŒì„± ì¸ì‹ ì‹œì‘
      SpeechRecognition.startListening({ 
        language: 'ko-KR', 
        continuous: true 
      });
    } else {
      // ìŒì†Œê±° - ìŒì„± ì¸ì‹ ì¤‘ì§€
      SpeechRecognition.stopListening();
      resetTranscript();
    }
  };

  const handleToggleSpeaker = () => {
    setIsSpeakerOn(!isSpeakerOn);
  };

  const handleStopListening = () => {
    console.log('â¹ï¸ User stopped listening');
    
    // ì¹¨ë¬µ íƒ€ì´ë¨¸ ì •ë¦¬
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    
    // í˜„ì¬ transcriptê°€ ìˆìœ¼ë©´ ì „ì†¡
    if (transcript && transcript !== lastSentTranscriptRef.current) {
      console.log('ğŸ“¤ Sending before stop:', transcript);
      wsRef.current?.sendTextInput(transcript);
      lastSentTranscriptRef.current = transcript;
      setTranscription(transcript);
    }
    
    SpeechRecognition.stopListening();
    resetTranscript();
  };

  return (
    <MobileLayout showNavBar={false}>
      <div 
        className="absolute inset-0 -mx-6 -mt-11 flex flex-col items-center justify-between"
        style={{
          background: 'linear-gradient(180deg, rgba(30,30,30,0.5) 0%, rgba(30,30,30,0.5) 100%)',
        }}
      >
        {/* ë°°ê²½ ì´ë¯¸ì§€ (ë¸”ëŸ¬ ì²˜ë¦¬) */}
        <div 
          className="absolute inset-0 -z-10"
          style={{
            backgroundImage: characterImage ? `url(${characterImage})` : 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)',
            backgroundSize: 'cover',
            backgroundPosition: 'center',
            filter: 'blur(40px)',
            transform: 'scale(1.2)',
          }}
        />

        {/* ìƒë‹¨ ì˜ì—­ - ë°œì‹ ì ì •ë³´ */}
        <div className="flex flex-col items-center pt-32 z-10 flex-1">
          {/* í”„ë¡œí•„ ì´ë¯¸ì§€ */}
          <div className="w-[120px] h-[120px] rounded-full bg-white flex items-center justify-center overflow-hidden shadow-2xl mb-6">
            {characterImage ? (
              <img
                src={characterImage}
                alt={characterName}
                className="w-full h-full object-cover"
              />
            ) : (
              <span className="text-[48px] text-[#FF7038] font-bold">
                {characterName[0]}
              </span>
            )}
          </div>

          {/* ë°œì‹ ì ì´ë¦„ */}
          <h1 className="text-[32px] font-bold text-white mb-2 drop-shadow-lg">
            {characterName}
          </h1>

          {/* ì „í™”ë²ˆí˜¸ */}
          {phoneNumber && (
            <p className="text-[18px] text-white opacity-70 mb-4 drop-shadow-md">
              {phoneNumber}
            </p>
          )}

          {/* í†µí™” ì‹œê°„ */}
          <p className="text-[24px] font-semibold text-white drop-shadow-lg">
            {formatDuration(callDuration)}
          </p>

          {/* ì‹¤ì‹œê°„ ì „ì‚¬ ë° AI ì‘ë‹µ í‘œì‹œ */}
          {(transcription || aiResponse || listening || isAiSpeaking) && (
            <div className="mt-8 px-8 max-w-md">
              {listening && !isAiSpeaking && (
                <div className="bg-red-500 bg-opacity-30 backdrop-blur-sm rounded-2xl p-4 mb-3 animate-pulse">
                  <p className="text-[14px] text-white opacity-90">
                    <span className="font-semibold">ğŸ™ï¸ ë“£ëŠ” ì¤‘...</span>
                  </p>
                </div>
              )}
              {transcription && (
                <div className="bg-white bg-opacity-20 backdrop-blur-sm rounded-2xl p-4 mb-3">
                  <p className="text-[14px] text-white opacity-90">
                    <span className="font-semibold">ì‚¬ìš©ì:</span> {transcription}
                  </p>
                </div>
              )}
              {isAiSpeaking && (
                <div className="bg-blue-500 bg-opacity-30 backdrop-blur-sm rounded-2xl p-4 mb-3 animate-pulse">
                  <p className="text-[14px] text-white opacity-90">
                    <span className="font-semibold">ğŸ”Š {characterName} ë§í•˜ëŠ” ì¤‘...</span>
                  </p>
                </div>
              )}
              {aiResponse && !isAiSpeaking && (
                <div className="bg-white bg-opacity-20 backdrop-blur-sm rounded-2xl p-4">
                  <p className="text-[14px] text-white opacity-90">
                    <span className="font-semibold">{characterName}:</span> {aiResponse}
                  </p>
                </div>
              )}
            </div>
          )}
        </div>

        {/* ìˆ¨ê²¨ì§„ ì˜¤ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ */}
        <audio ref={audioRef} hidden />

        {/* í•˜ë‹¨ ì˜ì—­ - ì»¨íŠ¸ë¡¤ ë²„íŠ¼ */}
        <div className="w-full pb-20 z-10">
          {/* ì¸ë””ì¼€ì´í„° */}
          <div className="flex justify-center mb-6">
            <div className="w-9 h-1 bg-white rounded-full opacity-60" />
          </div>

          {/* ë²„íŠ¼ ê·¸ë£¹ */}
          <div className="flex items-center justify-center gap-6 px-8 mb-8">
            {/* ìŒì†Œê±° ë²„íŠ¼ */}
            <div className="flex flex-col items-center gap-2">
              <button
                onClick={handleToggleMute}
                className={`w-14 h-14 rounded-full flex items-center justify-center transition-all shadow-lg ${
                  isMuted ? 'bg-[#838080] bg-opacity-100' : 'bg-white bg-opacity-16'
                }`}
                aria-label="ìŒì†Œê±°"
              >
                <img
                  src={isMuted ? "/icon/no_mike.svg" : "/icon/mike.svg"}
                  alt="ìŒì†Œê±°"
                  className="w-7 h-7 transition-all duration-300"
                  style={{
                    filter: isMuted ? 'brightness(0) invert(1)' : 'brightness(0) saturate(100%) invert(60%) sepia(0%) saturate(0%)'
                  }}
                />
              </button>
              <span className="text-[12px] text-white font-normal">
                ìŒì†Œê±°
              </span>
            </div>

            {/* ë…¹ìŒ ì¤‘ë‹¨ ë²„íŠ¼ (ë“£ëŠ” ì¤‘ì¼ ë•Œë§Œ í‘œì‹œ) */}
            {listening && !isAiSpeaking && (
              <div className="flex flex-col items-center gap-2">
                <button
                  onClick={handleStopListening}
                  className="w-14 h-14 bg-red-500 rounded-full flex items-center justify-center transition-all shadow-lg hover:bg-red-600 active:scale-95 animate-pulse"
                  aria-label="ë“£ê¸° ì¤‘ë‹¨"
                >
                  <div className="w-4 h-4 bg-white rounded-sm" />
                </button>
                <span className="text-[12px] text-white font-normal">
                  ì¤‘ë‹¨
                </span>
              </div>
            )}

            {/* ìŠ¤í”¼ì»¤ ë²„íŠ¼ */}
            <div className="flex flex-col items-center gap-2">
              <button
                onClick={handleToggleSpeaker}
                className={`w-14 h-14 rounded-full flex items-center justify-center transition-all duration-300 shadow-lg ${
                  isSpeakerOn
                    ? 'bg-[#838080] bg-opacity-100'
                    : 'bg-white bg-opacity-16'
                }`}
                aria-label="ìŠ¤í”¼ì»¤"
              >
                <img
                  src="/icon/speacker.svg"
                  alt="ìŠ¤í”¼ì»¤"
                  className="w-7 h-7 transition-all duration-300"
                  style={{
                    filter: isSpeakerOn ? 'brightness(0) invert(1)' : 'brightness(0) saturate(100%) invert(60%) sepia(0%) saturate(0%)'
                  }}
                />
              </button>
              <span className="text-[12px] text-white font-normal">
                ìŠ¤í”¼ì»¤
              </span>
            </div>

            {/* ì¢…ë£Œ ë²„íŠ¼ */}
            <div className="flex flex-col items-center gap-2">
              <button
                onClick={handleEndCall}
                className="w-14 h-14 bg-[#EB5545] rounded-full flex items-center justify-center transition-all shadow-lg hover:bg-[#D94A3C] active:scale-95"
                aria-label="í†µí™” ì¢…ë£Œ"
              >
                <img
                  src="/icon/end.svg"
                  alt="ì¢…ë£Œ"
                  className="w-8 h-8"
                  style={{ filter: 'brightness(0) invert(1)' }}
                />
              </button>
              <span className="text-[12px] text-white font-normal">
                end
              </span>
            </div>
          </div>
        </div>
      </div>
    </MobileLayout>
  );
}
